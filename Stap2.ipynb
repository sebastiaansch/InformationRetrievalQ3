{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-832e60f4311d>\", line 3, in <module>\n",
      "    traininglog[0] = traininglog[0].str.replace('[\\W_]+', ' ')\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py\", line 2534, in replace\n",
      "    flags=flags, regex=regex)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py\", line 582, in str_replace\n",
      "    return _na_map(f, arr)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py\", line 59, in _na_map\n",
      "    return _map(f, arr, na_mask=True, na_value=na_result, dtype=dtype)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py\", line 74, in _map\n",
      "    result = lib.map_infer_mask(arr, f, mask.view(np.uint8), convert)\n",
      "  File \"pandas/_libs/lib.pyx\", line 2175, in pandas._libs.lib.map_infer_mask\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py\", line 570, in <lambda>\n",
      "    f = lambda x: compiled.sub(repl=repl, string=x, count=n)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 179, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "suffixes = pd.read_csv(\"data/Freq_background.csv\", index_col='Unnamed: 0')\n",
    "traininglog = pd.read_csv(\"data/removed_session_background.csv\", header=None)\n",
    "traininglog[0] = traininglog[0].str.replace('[\\W_]+', ' ')\n",
    "sortedpopular = traininglog.groupby([0]).size().sort_values(ascending=False)\n",
    "sortedpopular = sortedpopular.reset_index(name=\"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100k = suffixes.iloc[range(100000),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: www.google\n",
      "['www', 'google']\n",
      "Endterm -> 'google'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top100k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6d29550fe026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Endterm -> '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mendterm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop100k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop100k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'counts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msortedpopular\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msortedpopular\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'counts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top100k' is not defined"
     ]
    }
   ],
   "source": [
    "query = input(\"Query: \")\n",
    "search_query = re.sub('[\\W_]+',' ', query)\n",
    "startt = time.time()\n",
    "splitted = re.split('[\\W_]+',search_query)\n",
    "print(splitted)\n",
    "if query[len(query)-1] == ' ':\n",
    "    endterm = splitted[len(splitted)-2] + \" \"\n",
    "else:\n",
    "    endterm = splitted[len(splitted)-1]\n",
    "\n",
    "print(\"Endterm -> '\" + endterm + \"'\")\n",
    "\n",
    "out = top100k[top100k['0'].str.startswith(endterm)].nlargest(10, 'counts')\n",
    "out2 = sortedpopular[sortedpopular[0].str.startswith(search_query)].nlargest(10, 'counts')\n",
    "\n",
    "outcombined1 = query[:-len(endterm)] + out['0'].astype(str)\n",
    "time.time() - startt\n",
    "\n",
    "print(outcombined1.append(out2.iloc[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = 'homedepotopinion.com' \n",
    "traininglog[0] = re.sub('[\\W_]+',' ', test_query[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traininglog.to_csv('cleaned_queries_background.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedpopular.to_csv('data/sorted_popular_queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
