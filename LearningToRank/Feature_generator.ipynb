{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import pyltr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import dawg\n",
    "from nltk import ngrams\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def get_single_ngram_features(query_input, ngram_dict):\n",
    "    ngram_scores = []\n",
    "\n",
    "    for n in range(1,7):\n",
    "        sixgrams = ngrams(query_input.split(), n)\n",
    "        ngram_score = 0\n",
    "        for grams in sixgrams:\n",
    "            try:\n",
    "                ngram_score = ngram_score + float(ngram_dict[' '.join(grams)])\n",
    "            except:\n",
    "                pass\n",
    "        ngram_scores.append(ngram_score)\n",
    "    return ngram_scores\n",
    "\n",
    "def get_ngram_features(inputs, ngram_dict):\n",
    "    results = [[get_single_ngram_features(x, ngram_dict)] for x in inputs];\n",
    "    return results\n",
    "\n",
    "\n",
    "# Importing all the necessary dictionaries\n",
    "suffixes = pd.read_csv(\"../data/Freq_background.csv\", index_col='Unnamed: 0')\n",
    "with open('../data/sorted_popular_queries.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    skipheader = next(reader)\n",
    "    data=[tuple([line[1], int(line[2])]) for line in reader]\n",
    "sortedpopulardict = dawg.IntCompletionDAWG(data)\n",
    "\n",
    "with open('../data/ngram_dict.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    ngram_dict = {rows[0]:rows[1] for rows in reader}\n",
    "# prefix_suffix_pairs_background = pd.read_csv(\"../data/prefix_suffix_pairs.txt\")\n",
    "\n",
    "data = []\n",
    "\n",
    "top100k = suffixes.iloc[range(100000),:]\n",
    "suffixes = []\n",
    "top100ktuple = [tuple(x) for x in top100k.values]\n",
    "# top100ktuple = [tuple([line[\"0\"], int(line[\"counts\"])])]\n",
    "top100kdict = dawg.IntCompletionDAWG(top100ktuple)\n",
    "top100k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "removed_session_training = pd.read_csv('../data/removed_session_training.csv', header=None, index_col=[0])\n",
    "removed_session_training[1] = removed_session_training[1].str.replace('[\\W_]+', ' ')\n",
    "\n",
    "training_queries = removed_session_training[:100].reset_index(drop=True)\n",
    "candidate_prefixes = []\n",
    "right_query = []\n",
    "\n",
    "for j in range(len(training_queries)):\n",
    "    current_query = str(training_queries[1][j])\n",
    "    split_query = current_query.split(\" \")\n",
    "    suffix = ' '.join(split_query[1:])\n",
    "    prefix = split_query[0] + \" \"\n",
    "\n",
    "    for i in range(len(suffix)+1):\n",
    "        candidate_prefixes.append(prefix + suffix[:i])\n",
    "        right_query.append(current_query)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8a3f5b9b9540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Appending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mappend_time_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mall_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mrelevant_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moriginal_query\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_candidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mqids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcandidate_prefix_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "all_candidates = np.array([])\n",
    "relevant_candidate = np.array([])\n",
    "qids = np.array([])\n",
    "ngram_features = np.array([])\n",
    "\n",
    "time_taken_top100k = 0\n",
    "split_time = 0 \n",
    "appendtime = 0\n",
    "\n",
    "total_time_begin = time.time()\n",
    "\n",
    "for candidate_prefix_i in range(len(candidate_prefixes)):\n",
    "#     print(str(candidate_prefix_i) + '/' + str(len(candidate_prefixes)))\n",
    "    candidate_prefix = candidate_prefixes[candidate_prefix_i]\n",
    "    original_query = right_query[candidate_prefix_i]\n",
    "    \n",
    "#     Getting the endterm for every prefix\n",
    "    split_time_start = time.time()\n",
    "    splitted = re.split('[\\W_]+',candidate_prefix)\n",
    "    if candidate_prefix[len(candidate_prefix)-1] == ' ':\n",
    "        endterm = splitted[len(splitted)-2] + \" \"\n",
    "    else:\n",
    "        endterm = splitted[len(splitted)-1]\n",
    "    split_time = split_time + time.time() - split_time_start\n",
    "    \n",
    "    # Creating the synthetic candidates\n",
    "    start = time.time()\n",
    "    top10suffix = nlargest(10, top100kdict.items(prefix=candidate_prefix), key=itemgetter(1))\n",
    "    out = [i[0] for i in top10suffix]    \n",
    "    preprefix = candidate_prefix[:-len(endterm)]\n",
    "    outcombined1 = [preprefix + s for s in out]\n",
    "    time_taken_top100k = time_taken_top100k + time.time() - start\n",
    "    \n",
    "    # Getting the top 50 real queries\n",
    "    top50queries = nlargest(50, sortedpopulardict.items(prefix=candidate_prefix), key=itemgetter(1))\n",
    "    keys_pop = [i[0] for i in top50queries]\n",
    "    \n",
    "    current_candidates = outcombined1\n",
    "    current_candidates.extend(keys_pop)\n",
    "    \n",
    "    # Appending\n",
    "    append_time_start = time.time()\n",
    "    all_candidates.append(current_candidates)    \n",
    "    relevant_candidate.append([s == original_query for s in current_candidates])\n",
    "    qids.append(np.ones(len(current_candidates)) * candidate_prefix_i)\n",
    "    appendtime = appendtime + time.time() - append_time_start\n",
    "\n",
    "    ngram_features = np.append(ngram_features, get_ngram_features(current_candidates, ngram_dict))\n",
    "    \n",
    "total_time = time.time() - total_time_begin\n",
    "\n",
    "print(\"Total time: \" + str(total_time))\n",
    "print(\"Top100k suffix time: \" + str(time_taken_top100k))\n",
    "print(\"Total split time: \" + str(split_time))\n",
    "print(\"Total append time: \" + str(appendtime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = input(\"Query: \")\n",
    "# original_query = input(\"Original query: \")\n",
    "\n",
    "# search_query = re.sub('[\\W_]+',' ', query)\n",
    "# original_query = re.sub('[\\W_]+',' ', original_query)\n",
    "\n",
    "# startt = time.time()\n",
    "# splitted = re.split('[\\W_]+',search_query)\n",
    "# print(splitted)\n",
    "# if query[len(query)-1] == ' ':\n",
    "#     endterm = splitted[len(splitted)-2] + \" \"\n",
    "# else:\n",
    "#     endterm = splitted[len(splitted)-1]\n",
    "\n",
    "# print(\"Endterm -> '\" + endterm + \"'\")\n",
    "\n",
    "# out = top100k[top100k['0'].str.startswith(endterm)].nlargest(10, 'counts')\n",
    "# out2 = sortedpopular[sortedpopular['0'].str.startswith(search_query)].nlargest(40, 'counts')\n",
    "\n",
    "# outcombined1 = query[:-len(endterm)] + out['0'].astype(str)\n",
    "# time.time() - startt\n",
    "# all_candidates = outcombined1.append(out2.iloc[:,0])\n",
    "# relevant_candidate = (all_candidates == original_query).apply(float)\n",
    "\n",
    "# train_input = pd.DataFrame({\n",
    "#     'query': all_candidates,\n",
    "#     'relevant': relevant_candidate,\n",
    "#     'qid': 1\n",
    "# })\n",
    "  \n",
    "# train_input['ngram_features'] = get_ngram_features(all_candidates, ngram_dict)\n",
    "# print(train_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pyltr.metrics.err.ERR(highest_score=1)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "TX2 = ngram_features.reshape(-1, 6) \n",
    "Ty2 = np.array(relevant_candidate)\n",
    "Tqids2 = np.array(qids)\n",
    "model.fit(TX2, Ty2, Tqids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train.txt') as trainfile, \\\n",
    "        open('../data/vali.txt') as valifile, \\\n",
    "        open('../data/test.txt') as evalfile:\n",
    "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EX = TX2 \n",
    "Ey = Ty2\n",
    "Eqids = Tqids2\n",
    "\n",
    "Epred = model.predict(EX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random ranking:' + str(metric.calc_mean_random(Eqids, Ey)))\n",
    "print('Our model:' + str(metric.calc_mean(Eqids, Ey, Epred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['hello kitty', 'hello my name is jeff', 'hello i am martijn', 'hello who are you', 'hello goodbye']\n",
    "EX = get_ngram_features(inputs,ngram_dict)\n",
    "EX = np.append([], EX)\n",
    "EX = EX.reshape(-1, 6) \n",
    "Ey = np.array([1, 0, 0, 0, 0])\n",
    "Eqids = np.ones(4)\n",
    "\n",
    "Epred = model.predict(EX)\n",
    "print(Epred)\n",
    "print('Random ranking:' + str(metric.calc_mean_random(Eqids, Ey)))\n",
    "print('Our model:' + str(metric.calc_mean(Eqids, Ey, Epred)))\n",
    "output = pd.DataFrame({\n",
    "    'inputs': inputs,\n",
    "    'value': Epred\n",
    "})\n",
    "output.sort_values('value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_candidate[relevant_candidate == 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
