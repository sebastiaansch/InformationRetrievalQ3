{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MRR\n",
    "import csv\n",
    "import time\n",
    "import dawg\n",
    "import pickle\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_lines = 106161931\n",
    "num_validation_lines = 102964961\n",
    "amount_of_divisions = 20\n",
    "\n",
    "preallocate_training = int(num_training_lines / (amount_of_divisions))\n",
    "preallocate_validation = int(num_validation_lines / (amount_of_divisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7b9dd2a1b46f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mskipheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msortedpopulardict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdawg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntDAWG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Maak klaar voor andere feature krijgen\n",
    "\n",
    "with open('../data/sorted_popular_queries.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    skipheader = next(reader)\n",
    "    data=[tuple([line[1], int(line[2])]) for line in reader if int(line[2])>2]\n",
    "sortedpopulardict = dawg.IntCompletionDAWG(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Reading possible candidates\n",
    "disable_synthetic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = pd.read_csv(\"../data/training_feature_data.csv\", header=None)\n",
    "training_file = \"../data/training_feature_data.csv\"\n",
    "start_time = time.time()\n",
    "training_ngram_features = []\n",
    "training_ylabels = []\n",
    "training_qids = []\n",
    "\n",
    "training_ngram_features=preallocate_training*[[0,0,0,0,0,0]]\n",
    "training_ylabels=preallocate_training*[np.bool(0)]\n",
    "training_qids=preallocate_training*[np.int(0)]\n",
    "training_popular_count_feature = preallocate_training*[np.int(0)]\n",
    "training_word_length_feature = preallocate_training*[np.int(0)]\n",
    "training_char_length_feature = preallocate_training*[np.int(0)]\n",
    "training_lastspace_feature=preallocate_training*[np.bool(0)]\n",
    "\n",
    "i = 0\n",
    "with open(training_file, newline='\\n') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';')\n",
    "    prev_qid = -1\n",
    "    current_qid = -1\n",
    "    split_counter = 0\n",
    "    \n",
    "    for row in spamreader:\n",
    "        if int(row[3]) % amount_of_divisions == 0:\n",
    "            continue\n",
    "            \n",
    "        # Skip if not in popular queries\n",
    "        if disable_synthetic:\n",
    "            if sortedpopulardict.get(row[2], 0) == 0:\n",
    "                continue\n",
    "            \n",
    "        # Get features\n",
    "        training_ngram_features[i] = eval(row[0])\n",
    "        training_popular_count_feature[i] = sortedpopulardict.get(row[2], 0)\n",
    "        training_word_length_feature[i] = len(row[2].split())\n",
    "        training_char_length_feature[i] = len(row[2])\n",
    "        training_lastspace_feature[i] = row[2][training_char_length_feature[i] - 1] == ' '\n",
    "\n",
    "        # Set question and label\n",
    "        training_ylabels[i] = np.bool(int(row[1]))\n",
    "        training_qids[i] = int(row[3])\n",
    "        i = i + 1\n",
    "        if i == preallocate_training:\n",
    "            break\n",
    "            \n",
    "        if i % 1000000 == 0:\n",
    "            print(str(i) + \"/ \" + str(preallocate_training))\n",
    "        \n",
    "            \n",
    "training_ngram_ints = np.array(training_ngram_features)\n",
    "training_ylabels_ready = np.array(training_ylabels)\n",
    "training_qids_ready = np.array(training_qids)\n",
    "\n",
    "training_ngram_features = []\n",
    "training_ylabels = []\n",
    "training_qids = []\n",
    "\n",
    "print(\"Dit koste: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_data = pd.read_csv(\"../data/validation_feature_data.csv\", header=None)\n",
    "validation_file = \"../data/validation_feature_data.csv\"\n",
    "start_time = time.time()\n",
    "validation_ngram_features = []\n",
    "validation_ylabels = []\n",
    "validation_qids = []\n",
    "\n",
    "validation_ngram_features=preallocate_validation*[[0,0,0,0,0,0]]\n",
    "validation_ylabels=preallocate_validation*[np.bool(0)]\n",
    "validation_qids=preallocate_validation*[np.int(0)]\n",
    "validation_popular_count_feature = preallocate_validation*[np.int(0)]\n",
    "validation_word_length_feature = preallocate_validation*[np.int(0)]\n",
    "validation_char_length_feature = preallocate_validation*[np.int(0)]\n",
    "validation_lastspace_feature=preallocate_validation*[np.bool(0)]\n",
    "\n",
    "i = 0\n",
    "with open(validation_file, newline='\\n') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';')\n",
    "    prev_qid = -1\n",
    "    current_qid = -1\n",
    "    split_counter = 0\n",
    "    \n",
    "    for row in spamreader:\n",
    "        if int(row[3]) % amount_of_divisions == 0:\n",
    "            continue\n",
    "        \n",
    "        # Skip if not in popular queries\n",
    "        if disable_synthetic:\n",
    "            if sortedpopulardict.get(row[2], 0) == 0:\n",
    "                continue\n",
    "                \n",
    "        # Get features\n",
    "        validation_ngram_features[i] = eval(row[0])\n",
    "        validation_popular_count_feature[i] = sortedpopulardict.get(row[2], 0)\n",
    "        validation_word_length_feature[i] = len(row[2].split())\n",
    "        validation_char_length_feature[i] = len(row[2])\n",
    "        validation_lastspace_feature[i] = row[2][validation_char_length_feature[i] - 1] == ' '\n",
    "\n",
    "        # Set question and label\n",
    "        validation_ylabels[i] = np.bool(int(row[1]))\n",
    "        validation_qids[i] = int(row[3])\n",
    "        i = i + 1\n",
    "        if i == preallocate_validation:\n",
    "            break\n",
    "        \n",
    "        if i % 1000000 == 0:\n",
    "            print(str(i) + \"/ \" + str(preallocate_validation))\n",
    "            \n",
    "validation_ngram_ints = np.array(validation_ngram_features)\n",
    "validation_ylabels_ready = np.array(validation_ylabels)\n",
    "validation_qids_ready = np.array(validation_qids)\n",
    "\n",
    "validation_ngram_features = []\n",
    "validation_ylabels = []\n",
    "validation_qids = []\n",
    "\n",
    "print(\"Dit koste: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/ 5148248\n",
      "2000000/ 5148248\n",
      "3000000/ 5148248\n",
      "4000000/ 5148248\n",
      "5000000/ 5148248\n",
      "6000000/ 5148248\n",
      "7000000/ 5148248\n",
      "8000000/ 5148248\n",
      "9000000/ 5148248\n",
      "10000000/ 5148248\n",
      "11000000/ 5148248\n",
      "12000000/ 5148248\n",
      "13000000/ 5148248\n",
      "14000000/ 5148248\n",
      "15000000/ 5148248\n",
      "16000000/ 5148248\n",
      "17000000/ 5148248\n",
      "18000000/ 5148248\n",
      "19000000/ 5148248\n",
      "20000000/ 5148248\n",
      "21000000/ 5148248\n",
      "22000000/ 5148248\n",
      "23000000/ 5148248\n",
      "24000000/ 5148248\n",
      "Dit koste: 353.3374288082123\n"
     ]
    }
   ],
   "source": [
    "# # testing_data = pd.read_csv(\"../data/testing_feature_data.csv\", header=None)\n",
    "testing_file = \"../data/testing_feature_data.csv\"\n",
    "start_time = time.time()\n",
    "testing_ngram_features = []\n",
    "testing_ylabels = []\n",
    "testing_qids = []\n",
    "preallocate_testing = 26821328\n",
    "testing_ngram_features=preallocate_testing*[[0,0,0,0,0,0]]\n",
    "testing_ylabels=preallocate_testing*[np.bool(0)]\n",
    "testing_qids=preallocate_testing*[np.int(0)]\n",
    "testing_popular_count_feature = preallocate_testing*[np.int(0)]\n",
    "testing_word_length_feature = preallocate_testing*[np.int(0)]\n",
    "testing_char_length_feature = preallocate_testing*[np.int(0)]\n",
    "testing_lastspace_feature=preallocate_testing*[np.bool(0)]\n",
    "\n",
    "i = 0\n",
    "with open(testing_file, newline='\\n') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';')\n",
    "    prev_qid = -1\n",
    "    current_qid = -1\n",
    "    split_counter = 0\n",
    "    \n",
    "    for row in spamreader:\n",
    "        # Get features\n",
    "        \n",
    "        # Skip if not in popular queries\n",
    "        if disable_synthetic:\n",
    "            if sortedpopulardict.get(row[2], 0) == 0:\n",
    "                continue\n",
    "        \n",
    "        testing_ngram_features[i] = eval(row[0])\n",
    "        testing_popular_count_feature[i] = sortedpopulardict.get(row[2], 0)\n",
    "        testing_word_length_feature[i] = len(row[2].split())\n",
    "        testing_char_length_feature[i] = len(row[2])\n",
    "        testing_lastspace_feature[i] = row[2][testing_char_length_feature[i] - 1] == ' '\n",
    "\n",
    "        # Set question and label\n",
    "        testing_ylabels[i] = np.bool(int(row[1]))\n",
    "        testing_qids[i] = int(row[3])\n",
    "        i = i + 1\n",
    "        if i % 1000000 == 0:\n",
    "            print(str(i) + \"/ \" + str(preallocate_validation))\n",
    "            \n",
    "testing_ngram_ints = np.array(testing_ngram_features)\n",
    "testing_ylabels_ready = np.array(testing_ylabels)\n",
    "testing_qids_ready = np.array(testing_qids)\n",
    "\n",
    "testing_ngram_features = []\n",
    "testing_ylabels = []\n",
    "testing_qids = []\n",
    "\n",
    "print(\"Dit koste: \" + str(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select the features\n",
    "ngram_features = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all the features, ngram, word length, char length, popularity & last space\n",
    "\n",
    "if ngram_features:\n",
    "    training_features_final = np.ndarray(shape=(len(training_ngram_ints), 10))\n",
    "    training_features_final[:,0:6] = training_ngram_ints\n",
    "    training_features_final[:,6] = training_popular_count_feature\n",
    "    training_features_final[:,7] = training_word_length_feature\n",
    "    training_features_final[:,8] = training_char_length_feature\n",
    "    training_features_final[:,9] = training_lastspace_feature\n",
    "else:\n",
    "    training_features_final = np.ndarray(shape=(len(training_ngram_ints), 4))\n",
    "    training_features_final[:,0] = training_popular_count_feature\n",
    "    training_features_final[:,1] = training_word_length_feature\n",
    "    training_features_final[:,2] = training_char_length_feature\n",
    "    training_features_final[:,3] = training_lastspace_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all the features, ngram, word length, char length, popularity & last space\n",
    "\n",
    "if ngram_features:\n",
    "    validation_features_final = np.ndarray(shape=(len(validation_ngram_ints), 10))\n",
    "    validation_features_final[:,0:6] = validation_ngram_ints\n",
    "    validation_features_final[:,6] = validation_popular_count_feature\n",
    "    validation_features_final[:,7] = validation_word_length_feature\n",
    "    validation_features_final[:,8] = validation_char_length_feature\n",
    "    validation_features_final[:,9] = validation_lastspace_feature\n",
    "else:\n",
    "    validation_features_final = np.ndarray(shape=(len(validation_ngram_ints), 4))\n",
    "    validation_features_final[:,0] = validation_popular_count_feature\n",
    "    validation_features_final[:,1] = validation_word_length_feature\n",
    "    validation_features_final[:,2] = validation_char_length_feature\n",
    "    validation_features_final[:,3] = validation_lastspace_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all the features, ngram, word length, char length, popularity & last space\n",
    "\n",
    "if ngram_features:\n",
    "    testing_features_final = np.ndarray(shape=(len(testing_ngram_ints), 10))\n",
    "    testing_features_final[:,0:6] = testing_ngram_ints\n",
    "    testing_features_final[:,6] = testing_popular_count_feature\n",
    "    testing_features_final[:,7] = testing_word_length_feature\n",
    "    testing_features_final[:,8] = testing_char_length_feature\n",
    "    testing_features_final[:,9] = testing_lastspace_feature\n",
    "else:\n",
    "    testing_features_final = np.ndarray(shape=(len(testing_ngram_ints), 4))\n",
    "    testing_features_final[:,0] = testing_popular_count_feature\n",
    "    testing_features_final[:,1] = testing_word_length_feature\n",
    "    testing_features_final[:,2] = testing_char_length_feature\n",
    "    testing_features_final[:,3] = testing_lastspace_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Model training - Currently, validation is disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score    Remaining                           Monitor Output \n",
      "    1       0.1965       10.22m                                         \n",
      "    2       0.1981        7.77m                                         \n",
      "    3       0.1984        5.21m                                         \n",
      "    4       0.1991        2.68m                                         \n",
      "    5       0.2000        0.00s                                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyltr.models.lambdamart.LambdaMART at 0x7f946dfb7d68>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = pyltr.metrics.ERR(k=10, highest_score = 1)\n",
    "metric_MRR = MRR.MRR(k=10, highest_score = 1)\n",
    "\n",
    "# Only needed if you want to perform validation (early stopping & trimming)\n",
    "# monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "#     validation_features_final, validation_ylabels_ready, validation_qids_ready, metric=metric, stop_after=5)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# model.fit(training_features_final, training_ylabels_ready, training_qids_ready, monitor=monitor)\n",
    "model.fit(training_features_final, training_ylabels_ready, training_qids_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=True,\n",
       "           random_state=<mtrand.RandomState object at 0x7f94a6cb71b0>,\n",
       "           splitter='best')],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=True,\n",
       "           random_state=<mtrand.RandomState object at 0x7f94a6cb71b0>,\n",
       "           splitter='best')],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=True,\n",
       "           random_state=<mtrand.RandomState object at 0x7f94a6cb71b0>,\n",
       "           splitter='best')],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=True,\n",
       "           random_state=<mtrand.RandomState object at 0x7f94a6cb71b0>,\n",
       "           splitter='best')],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=True,\n",
       "           random_state=<mtrand.RandomState object at 0x7f94a6cb71b0>,\n",
       "           splitter='best')]], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( model, open( \"model1top100k5iter_nongram.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model:0.22471519880928417\n"
     ]
    }
   ],
   "source": [
    "Epred = model.predict(testing_features_final)\n",
    "print('Our model:' +  str(metric.calc_mean(testing_qids_ready, testing_ylabels_ready, Epred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Our model MRR:0.4414690206869351\n"
     ]
    }
   ],
   "source": [
    "print(' Our model MRR:'+  str(metric_MRR.calc_mean(testing_qids_ready, testing_ylabels_ready, Epred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.803310e+06, 1.940000e+02, 5.200000e+01, ..., 3.000000e+00,\n",
       "        1.300000e+01, 0.000000e+00],\n",
       "       [4.802758e+06, 6.300000e+01, 1.900000e+01, ..., 3.000000e+00,\n",
       "        1.400000e+01, 0.000000e+00],\n",
       "       [4.802865e+06, 3.900000e+01, 1.600000e+01, ..., 3.000000e+00,\n",
       "        1.500000e+01, 0.000000e+00],\n",
       "       ...,\n",
       "       [2.759820e+05, 3.320000e+02, 0.000000e+00, ..., 5.000000e+00,\n",
       "        1.800000e+01, 0.000000e+00],\n",
       "       [2.059000e+04, 2.142000e+03, 0.000000e+00, ..., 2.000000e+00,\n",
       "        1.100000e+01, 0.000000e+00],\n",
       "       [2.620700e+04, 5.030000e+02, 0.000000e+00, ..., 2.000000e+00,\n",
       "        1.300000e+01, 0.000000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
